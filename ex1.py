import re
import sys
import random
import math
import collections
from collections import defaultdict


class Ngram_Language_Model:
    """The class implements a Markov Language Model that learns a language model
        from a given text.
        It supports language generation and the evaluation of a given string.
        The class can be applied on both word level and character level.
    """

    def __init__(self, n=3, chars=False):
        """Initializing a language model object.
        Args:
            n (int): the length of the markov unit (the n of the n-gram). Defaults to 3.
            chars (bool): True iff the model consists of ngrams of characters rather then word tokens.
                          Defaults to False
        """
        self.n = n
        self.model_dict = defaultdict(
            int)  # a dictionary of the form {ngram:count}, holding counts of all ngrams in the specified text.
        self.chars = chars
        self.vocabulary = None  # a set of the types in the text
        self.ngrams_dict = defaultdict(int)  # dictionary of ngrams dictionaries

    def build_model(self, text):
        """populates the instance variable model_dict.

            Args:
                text (str): the text to construct the model from.
        """
        tokens = re.split(r'\s+', text)  # create a list of words out of the corpora

        # Every tuple of n words is joined to a string, and the Counter func creates a dict with counts
        self.model_dict = defaultdict(int, collections.Counter(
            " ".join(tuple(tokens[i:i + self.n])) for i in range(len(tokens) - self.n+1)))

        # a dictionary of every possible n-gram dictionary.
        for j in range(self.n):
            self.ngrams_dict[self.n - j] = defaultdict(int, collections.Counter(
                " ".join(tuple(tokens[i:i + self.n - j])) for i in range(len(tokens) - (self.n - j))))


            # self.ngrams_dict[self.n - j][" ".join(tuple(tokens[len(tokens)-j:len(tokens)]))] -= 1

        self.vocabulary = set(tokens)

    def get_model_dictionary(self):
        """Returns the dictionary class object
        """
        return self.model_dict

    def get_model_window_size(self):
        """Returning the size of the context window (the n in "n-gram")
        """
        return self.n

    def P(self, candidate, context, given_n=None):
        sequence = context.copy()
        sequence.append(candidate)
        if given_n is None:
            return self.model_dict[" ".join(sequence)] / self.ngrams_dict[self.n - 1][" ".join(context)]

        return self.ngrams_dict[given_n][" ".join(sequence)] / self.ngrams_dict[given_n-1][" ".join(context)]


    def p_first(self, word):
        """Returns the probability for a given word to be the first in a context

            Args:
                word(str): the word to calculate its probability

            Return:
                p(float): probability to be first
         """
        # p = self.ngrams_dict[1][word] / sum(self.ngrams_dict[2].values())
        p = self.ngrams_dict[1][word] / sum(self.model_dict.values())
        return p

    def candidates(self, context, n_gram=None):
        """Returns a set of all possible ngrams sequences

            Args:
                context (list): the context to create candidates from
                n_gram(int): if generating for n_gram < self.n, generates for the relevant dictionary

            Return:
                List.
        """
        candi = set()  # initialize the candidates set

        for w in self.vocabulary:  # add every word in vocabulary to the given context
            c = context.copy()
            c.append(w)

            if n_gram is None:
                if " ".join(c) in self.model_dict: candi.add(w)    # verify this sequence is in the model dictionary
            else:
                if " ".join(c) in self.ngrams_dict[n_gram]: candi.add(w)

        # candi1 = set(context+" "+w for w in self.vocabulary if context+" "+w in self.model_dict)
        return candi

    def generate(self, context=None, n=20):
        """Returns a string of the specified length, generated by applying the language model
        to the specified seed context. If no context is specified the context should be sampled
        from the models' contexts distribution. Generation should stop before the n'th word if the
        contexts are exhausted. If the length of the specified context exceeds (or equal to)
        the specified n, the method should return the a prefix of length n of the specified context.

            Args:
                context (str): a seed context to start the generated string from. Defaults to None
                n (int): the length of the string to be generated.

            Return:
                String. The generated text.

        """

        if context == None: context=''
        if len(context)==0:
            context = random.choices(population=list(self.ngrams_dict[self.n-1].keys()), weights=self.ngrams_dict[self.n-1].values())[0]

        context_l = re.split(r'\s+', context)  # the context as a list of words
        str = context_l.copy()   # the generates string start for the context

        if len(context_l) >= n: return " ".join(context_l[0:n])

        # context is shorter than the needed length
        if len(context_l) < self.n - 1:
            ngram = context_l.copy()
            for current_n in range(len(context_l)+1, min(n, self.n)):  # generate enough words for context for ngram
                cands = self.candidates(ngram.copy(), current_n)
                if not cands: return " ".join(str)  # no possible candidates -> end function.
                chosen = self.choose(cands, ngram.copy(), current_n)  # choose the word with highest probability from the list of options for next word
                str.append(chosen)
                ngram.append(chosen)


        # context is longer than needed
        elif len(context_l) > self.n - 1:
            ngram = (context_l[len(context_l) - (self.n - 1):]).copy()

        # context is exactly self.n - 1
        else:
            ngram = context_l.copy()

        for i in range(0, n - len(context_l)):
            cands = self.candidates(ngram.copy())
            if not cands: break   # no possible candidates -> end function.
            chosen = self.choose(cands, ngram.copy())  # choose the word with highest probability from the list of options for next word
            str.append(chosen)
            ngram.append(chosen)
            ngram.pop(0)

        return " ".join(str)

    def choose(self, candidates, context, n_gram=None):
        """Return the word with the highest probability to be next in the sentence based on ngrams.
        If there are more than one word with the name probability, a random choice is made.

        Args:
            candidates (list): list of possible candidates, based on ngram algorithm

        Return:
            The chosen word (str)
        """
        probs = {}  # dictionary of the candidates with their probabilities
        for c in candidates:  # c is a tuple of (ngram list, predicted word)
            probs[c] = self.P(c, context, n_gram)  # calculate the probability for each candidate.

        return (random.choices(population=list(probs.keys()), weights=probs.values(), k=1))[0]

    def evaluate(self, text):
        """Returns the log-likelihood of the specified text to be a product of the model.
           Laplace smoothing should be applied if necessary.

           Args:
               text (str): Text to evaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        text = normalize_text(text)
        log_probs = []
        text_list = re.split(r'\s+', text)    # split the text to a list of words
        text_len = len(text_list)   # text word count
        if text_len == 0:   # no text to evaluate
            return  # TODO add throw exception

        log_probs.append(math.log(self.p_first(text_list[0])))   # calc first word based on the model's context distribution

        for n in range(2, min(text_len, self.n)):   # calc probability for words until there are enough for ngram according to self.n
            ngram = " ".join(text_list[0:n])
            nm_gram = " ".join(text_list[0:n - 1])
            log_probs.append(math.log(self.ngrams_dict[n][ngram] / self.ngrams_dict[n - 1][nm_gram]))

        if text_len >= self.n:
            for i in range(0, text_len - (self.n - 1)):
                ngram = " ".join(text_list[i:i + self.n])
                nm_gram = " ".join(text_list[i:i + self.n - 1])
                log_probs.append(math.log(self.model_dict[ngram] / self.ngrams_dict[self.n - 1][nm_gram]))
        # print(log_probs)
        return round(sum(log_probs), 3)

    def smooth(self, ngram):
        """Returns the smoothed (Laplace) probability of the specified ngram.

            Args:
                ngram (str): the ngram to have it's probability smoothed

            Returns:
                float. The smoothed probability.
        """


def normalize_text(text):
    """Returns a normalized version of the specified string.
      You can add default parameters as you like (they should have default values!)
      You should explain your decisions in the header of the function.

      Args:
        text (str): the text to normalize

      Returns:
        string. the normalized text.
    """
    # TODO add a case when the model is for chars and not words
    nt = text.lower()  # lower-case the text
    # nt = re.sub('([.,!?()%^$&-])', r' \1 ', nt)   # add space before/after a punctuation
    # nt = re.sub(r'\s+', ' ', nt)   # remove unwanted spaces (more than 1)
    nt = re.sub('(?<! )(?=[.,:?!@#$%^&*()\[\]\\\])|(?<=[.,:?!@#$%^&*()\[\]\\\])(?! )', r' ', nt)
    tokens = re.split(r'\s+', nt)  # create a list of words out of the corpora
    if tokens[-1] == '':
        tokens.pop()
        nt = " ".join(tokens)
    return nt


def who_am_i():  # this is not a class method
    """Returns a dictionary with your name, id number and email. keys=['name', 'id','email']
        Make sure you return your own info!
    """
    return {'name': 'Eyal Ginosar', 'id': '307830901', 'email': 'eyalgi@post.bgu.ac.il'}
